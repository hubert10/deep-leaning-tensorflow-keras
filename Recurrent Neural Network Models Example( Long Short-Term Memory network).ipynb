{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 5, 1) (12, 5, 1) (91,) (12,)\n",
      "Train on 91 samples, validate on 12 samples\n",
      "Epoch 1/350\n",
      "91/91 - 4s - loss: 1937745523.3407 - mae: 41331.9102 - val_loss: 1061044544.0000 - val_mae: 31229.7500\n",
      "Epoch 2/350\n",
      "91/91 - 0s - loss: 709610280.7912 - mae: 23914.7246 - val_loss: 218102736.0000 - val_mae: 13900.7969\n",
      "Epoch 3/350\n",
      "91/91 - 0s - loss: 157246349.5385 - mae: 9885.6758 - val_loss: 18893528.0000 - val_mae: 3628.5608\n",
      "Epoch 4/350\n",
      "91/91 - 0s - loss: 68687397.9780 - mae: 6231.5098 - val_loss: 132292328.0000 - val_mae: 9804.1172\n",
      "Epoch 5/350\n",
      "91/91 - 0s - loss: 124620085.9780 - mae: 9722.0566 - val_loss: 212861744.0000 - val_mae: 12210.5049\n",
      "Epoch 6/350\n",
      "91/91 - 0s - loss: 136877122.9890 - mae: 10271.1113 - val_loss: 180246144.0000 - val_mae: 11396.5576\n",
      "Epoch 7/350\n",
      "91/91 - 0s - loss: 108414441.2308 - mae: 9062.6865 - val_loss: 123084376.0000 - val_mae: 9120.6182\n",
      "Epoch 8/350\n",
      "91/91 - 0s - loss: 70699781.1429 - mae: 6960.0308 - val_loss: 52841324.0000 - val_mae: 5320.0962\n",
      "Epoch 9/350\n",
      "91/91 - 0s - loss: 36531092.9451 - mae: 4865.5283 - val_loss: 36376552.0000 - val_mae: 5105.2515\n",
      "Epoch 10/350\n",
      "91/91 - 0s - loss: 24864771.2308 - mae: 4083.8123 - val_loss: 52574224.0000 - val_mae: 6235.9243\n",
      "Epoch 11/350\n",
      "91/91 - 0s - loss: 28609379.7363 - mae: 4369.4741 - val_loss: 64914208.0000 - val_mae: 7138.2109\n",
      "Epoch 12/350\n",
      "91/91 - 0s - loss: 30708546.8352 - mae: 4457.7412 - val_loss: 70602328.0000 - val_mae: 7649.5249\n",
      "Epoch 13/350\n",
      "91/91 - 0s - loss: 32700336.9890 - mae: 4623.1948 - val_loss: 64508524.0000 - val_mae: 7304.8110\n",
      "Epoch 14/350\n",
      "91/91 - 0s - loss: 29880152.0000 - mae: 4413.7749 - val_loss: 54279292.0000 - val_mae: 6735.2085\n",
      "Epoch 15/350\n",
      "91/91 - 0s - loss: 25768665.3187 - mae: 4073.8870 - val_loss: 44242652.0000 - val_mae: 5957.2212\n",
      "Epoch 16/350\n",
      "91/91 - 0s - loss: 23573144.8571 - mae: 3936.5854 - val_loss: 34454804.0000 - val_mae: 5061.3647\n",
      "Epoch 17/350\n",
      "91/91 - 0s - loss: 20782398.6374 - mae: 3758.0095 - val_loss: 25323814.0000 - val_mae: 4290.8828\n",
      "Epoch 18/350\n",
      "91/91 - 0s - loss: 18244695.6154 - mae: 3485.8330 - val_loss: 18732202.0000 - val_mae: 3685.9314\n",
      "Epoch 19/350\n",
      "91/91 - 0s - loss: 17077934.1319 - mae: 3313.2188 - val_loss: 10876724.0000 - val_mae: 2966.0752\n",
      "Epoch 20/350\n",
      "91/91 - 0s - loss: 15736978.1648 - mae: 3169.9451 - val_loss: 10523604.0000 - val_mae: 2815.7822\n",
      "Epoch 21/350\n",
      "91/91 - 0s - loss: 15453959.2967 - mae: 3073.9470 - val_loss: 10842938.0000 - val_mae: 2716.8384\n",
      "Epoch 22/350\n",
      "91/91 - 0s - loss: 15434712.3516 - mae: 3061.3718 - val_loss: 11018015.0000 - val_mae: 2651.4436\n",
      "Epoch 23/350\n",
      "91/91 - 0s - loss: 14264428.8681 - mae: 2948.3447 - val_loss: 10835669.0000 - val_mae: 2654.1387\n",
      "Epoch 24/350\n",
      "91/91 - 0s - loss: 12639502.8791 - mae: 2799.1670 - val_loss: 13381619.0000 - val_mae: 2978.2566\n",
      "Epoch 25/350\n",
      "91/91 - 0s - loss: 12050041.6264 - mae: 2718.8337 - val_loss: 14043448.0000 - val_mae: 3105.2119\n",
      "Epoch 26/350\n",
      "91/91 - 0s - loss: 11609982.8462 - mae: 2659.0784 - val_loss: 10502011.0000 - val_mae: 2776.4841\n",
      "Epoch 27/350\n",
      "91/91 - 0s - loss: 11543676.6813 - mae: 2666.1953 - val_loss: 10123815.0000 - val_mae: 2680.5234\n",
      "Epoch 28/350\n",
      "91/91 - 0s - loss: 11389645.4835 - mae: 2672.9958 - val_loss: 9594827.0000 - val_mae: 2572.5300\n",
      "Epoch 29/350\n",
      "91/91 - 0s - loss: 11209784.4066 - mae: 2661.1514 - val_loss: 9768695.0000 - val_mae: 2588.2441\n",
      "Epoch 30/350\n",
      "91/91 - 0s - loss: 11086973.4396 - mae: 2642.6240 - val_loss: 9814656.0000 - val_mae: 2589.7019\n",
      "Epoch 31/350\n",
      "91/91 - 0s - loss: 10865342.6154 - mae: 2567.3665 - val_loss: 10258563.0000 - val_mae: 2623.0920\n",
      "Epoch 32/350\n",
      "91/91 - 0s - loss: 10803501.0330 - mae: 2562.5479 - val_loss: 10020382.0000 - val_mae: 2601.9963\n",
      "Epoch 33/350\n",
      "91/91 - 0s - loss: 10622088.0440 - mae: 2543.6807 - val_loss: 10069744.0000 - val_mae: 2604.9407\n",
      "Epoch 34/350\n",
      "91/91 - 0s - loss: 10519483.5385 - mae: 2529.0686 - val_loss: 11819992.0000 - val_mae: 2841.5774\n",
      "Epoch 35/350\n",
      "91/91 - 0s - loss: 10513027.5055 - mae: 2547.4170 - val_loss: 11005881.0000 - val_mae: 2727.5618\n",
      "Epoch 36/350\n",
      "91/91 - 0s - loss: 10198610.9560 - mae: 2510.5864 - val_loss: 12967221.0000 - val_mae: 2904.2258\n",
      "Epoch 37/350\n",
      "91/91 - 0s - loss: 10783478.1978 - mae: 2513.7239 - val_loss: 9852618.0000 - val_mae: 2501.7478\n",
      "Epoch 38/350\n",
      "91/91 - 0s - loss: 9922630.3516 - mae: 2435.9209 - val_loss: 9188489.0000 - val_mae: 2504.7141\n",
      "Epoch 39/350\n",
      "91/91 - 0s - loss: 9601266.4066 - mae: 2409.0657 - val_loss: 8554819.0000 - val_mae: 2442.7161\n",
      "Epoch 40/350\n",
      "91/91 - 0s - loss: 9835113.8681 - mae: 2461.5625 - val_loss: 8307309.5000 - val_mae: 2410.8127\n",
      "Epoch 41/350\n",
      "91/91 - 0s - loss: 9872905.0330 - mae: 2509.4443 - val_loss: 9115615.0000 - val_mae: 2616.2102\n",
      "Epoch 42/350\n",
      "91/91 - 0s - loss: 9847770.4725 - mae: 2515.4014 - val_loss: 9248968.0000 - val_mae: 2638.3347\n",
      "Epoch 43/350\n",
      "91/91 - 0s - loss: 9784860.5165 - mae: 2508.2529 - val_loss: 9323606.0000 - val_mae: 2659.0908\n",
      "Epoch 44/350\n",
      "91/91 - 0s - loss: 9492282.7308 - mae: 2449.2639 - val_loss: 9434471.0000 - val_mae: 2681.7434\n",
      "Epoch 45/350\n",
      "91/91 - 0s - loss: 9312882.3956 - mae: 2445.9629 - val_loss: 9390193.0000 - val_mae: 2678.9177\n",
      "Epoch 46/350\n",
      "91/91 - 0s - loss: 9146703.8462 - mae: 2429.6375 - val_loss: 9355440.0000 - val_mae: 2668.3694\n",
      "Epoch 47/350\n",
      "91/91 - 0s - loss: 9108221.5604 - mae: 2378.4788 - val_loss: 14566491.0000 - val_mae: 3094.7776\n",
      "Epoch 48/350\n",
      "91/91 - 0s - loss: 8253484.3077 - mae: 2307.0425 - val_loss: 12218735.0000 - val_mae: 2785.1111\n",
      "Epoch 49/350\n",
      "91/91 - 0s - loss: 9340859.1209 - mae: 2398.0256 - val_loss: 25677216.0000 - val_mae: 3773.9109\n",
      "Epoch 50/350\n",
      "91/91 - 0s - loss: 10757932.6813 - mae: 2546.3616 - val_loss: 17848026.0000 - val_mae: 3109.6575\n",
      "Epoch 51/350\n",
      "91/91 - 0s - loss: 10171235.9121 - mae: 2382.7578 - val_loss: 11978304.0000 - val_mae: 2741.3196\n",
      "Epoch 52/350\n",
      "91/91 - 0s - loss: 8929583.2198 - mae: 2436.0610 - val_loss: 11358909.0000 - val_mae: 2687.9495\n",
      "Epoch 53/350\n",
      "91/91 - 0s - loss: 9141994.9560 - mae: 2404.5430 - val_loss: 12028485.0000 - val_mae: 2765.7168\n",
      "Epoch 54/350\n",
      "91/91 - 0s - loss: 8596933.9451 - mae: 2289.8533 - val_loss: 13939707.0000 - val_mae: 3169.6296\n",
      "Epoch 55/350\n",
      "91/91 - 0s - loss: 8169913.4396 - mae: 2222.2229 - val_loss: 14069389.0000 - val_mae: 3041.9534\n",
      "Epoch 56/350\n",
      "91/91 - 0s - loss: 8136291.8901 - mae: 2198.6038 - val_loss: 14022640.0000 - val_mae: 2848.8201\n",
      "Epoch 57/350\n",
      "91/91 - 0s - loss: 8062606.0440 - mae: 2201.7388 - val_loss: 12642987.0000 - val_mae: 2739.4441\n",
      "Epoch 58/350\n",
      "91/91 - 0s - loss: 7879403.1429 - mae: 2194.2761 - val_loss: 11806008.0000 - val_mae: 2691.4763\n",
      "Epoch 59/350\n",
      "91/91 - 0s - loss: 7487389.3187 - mae: 2137.0740 - val_loss: 12205401.0000 - val_mae: 2937.6511\n",
      "Epoch 60/350\n",
      "91/91 - 0s - loss: 7724457.1813 - mae: 2172.8970 - val_loss: 12778632.0000 - val_mae: 3087.3425\n",
      "Epoch 61/350\n",
      "91/91 - 0s - loss: 7483650.3187 - mae: 2127.6816 - val_loss: 12135283.0000 - val_mae: 2837.0781\n",
      "Epoch 62/350\n",
      "91/91 - 0s - loss: 7435090.6648 - mae: 2110.1416 - val_loss: 11636088.0000 - val_mae: 2686.2627\n",
      "Epoch 63/350\n",
      "91/91 - 0s - loss: 7420802.8626 - mae: 2089.0073 - val_loss: 12431913.0000 - val_mae: 2801.0461\n",
      "Epoch 64/350\n",
      "91/91 - 0s - loss: 7344934.1648 - mae: 2084.5166 - val_loss: 14718052.0000 - val_mae: 3206.6111\n",
      "Epoch 65/350\n",
      "91/91 - 0s - loss: 7281868.2967 - mae: 2063.7227 - val_loss: 14935409.0000 - val_mae: 3241.2649\n",
      "Epoch 66/350\n",
      "91/91 - 0s - loss: 7282324.0769 - mae: 2064.7334 - val_loss: 14570699.0000 - val_mae: 3119.3562\n",
      "Epoch 67/350\n",
      "91/91 - 0s - loss: 7267902.3516 - mae: 2064.2527 - val_loss: 13783215.0000 - val_mae: 2997.7747\n",
      "Epoch 68/350\n",
      "91/91 - 0s - loss: 7243739.8901 - mae: 2086.3860 - val_loss: 13927141.0000 - val_mae: 3081.5420\n",
      "Epoch 69/350\n",
      "91/91 - 0s - loss: 7311244.7088 - mae: 2079.7949 - val_loss: 15116732.0000 - val_mae: 3353.0300\n",
      "Epoch 70/350\n",
      "91/91 - 0s - loss: 7148776.1044 - mae: 2044.8282 - val_loss: 14493204.0000 - val_mae: 3219.2920\n",
      "Epoch 71/350\n",
      "91/91 - 0s - loss: 7157121.2363 - mae: 2030.0653 - val_loss: 13762571.0000 - val_mae: 3027.2219\n",
      "Epoch 72/350\n",
      "91/91 - 0s - loss: 7049648.8462 - mae: 2021.6440 - val_loss: 13716047.0000 - val_mae: 3027.4612\n",
      "Epoch 73/350\n",
      "91/91 - 0s - loss: 7021943.3681 - mae: 2014.3907 - val_loss: 14103125.0000 - val_mae: 3132.8860\n",
      "Epoch 74/350\n",
      "91/91 - 0s - loss: 7062989.5385 - mae: 2026.8992 - val_loss: 14468664.0000 - val_mae: 3194.4226\n",
      "Epoch 75/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 - 0s - loss: 7089007.2363 - mae: 2026.4739 - val_loss: 13735121.0000 - val_mae: 3042.1160\n",
      "Epoch 76/350\n",
      "91/91 - 0s - loss: 6928684.1868 - mae: 1997.9921 - val_loss: 13983293.0000 - val_mae: 3076.3516\n",
      "Epoch 77/350\n",
      "91/91 - 0s - loss: 6941450.3626 - mae: 1993.8137 - val_loss: 13990864.0000 - val_mae: 3102.5105\n",
      "Epoch 78/350\n",
      "91/91 - 0s - loss: 6920954.6538 - mae: 1997.2822 - val_loss: 13942219.0000 - val_mae: 3097.0332\n",
      "Epoch 79/350\n",
      "91/91 - 0s - loss: 6888590.5549 - mae: 1992.7703 - val_loss: 13862192.0000 - val_mae: 3080.7344\n",
      "Epoch 80/350\n",
      "91/91 - 0s - loss: 6898947.1978 - mae: 1994.8116 - val_loss: 13894869.0000 - val_mae: 3080.1584\n",
      "Epoch 81/350\n",
      "91/91 - 0s - loss: 6912003.7527 - mae: 1998.4283 - val_loss: 13995651.0000 - val_mae: 3104.8860\n",
      "Epoch 82/350\n",
      "91/91 - 0s - loss: 6895140.5385 - mae: 2003.6343 - val_loss: 14414472.0000 - val_mae: 3179.8691\n",
      "Epoch 83/350\n",
      "91/91 - 0s - loss: 6850687.7857 - mae: 1995.2373 - val_loss: 14259517.0000 - val_mae: 3129.5266\n",
      "Epoch 84/350\n",
      "91/91 - 0s - loss: 6894677.9066 - mae: 1988.1403 - val_loss: 13807807.0000 - val_mae: 3014.7278\n",
      "Epoch 85/350\n",
      "91/91 - 0s - loss: 7014674.1484 - mae: 2009.3300 - val_loss: 13461536.0000 - val_mae: 3004.1689\n",
      "Epoch 86/350\n",
      "91/91 - 0s - loss: 6979494.1703 - mae: 2021.4818 - val_loss: 14075296.0000 - val_mae: 3140.7041\n",
      "Epoch 87/350\n",
      "91/91 - 0s - loss: 6926503.9451 - mae: 1995.4553 - val_loss: 13819340.0000 - val_mae: 3063.9893\n",
      "Epoch 88/350\n",
      "91/91 - 0s - loss: 7106160.1209 - mae: 1999.5736 - val_loss: 14187379.0000 - val_mae: 3104.5969\n",
      "Epoch 89/350\n",
      "91/91 - 0s - loss: 6785856.0549 - mae: 1971.1516 - val_loss: 13494859.0000 - val_mae: 3021.9792\n",
      "Epoch 90/350\n",
      "91/91 - 0s - loss: 6887038.0000 - mae: 2000.6241 - val_loss: 13477411.0000 - val_mae: 3000.0332\n",
      "Epoch 91/350\n",
      "91/91 - 0s - loss: 6827074.1813 - mae: 1990.3475 - val_loss: 13823883.0000 - val_mae: 3091.9226\n",
      "Epoch 92/350\n",
      "91/91 - 0s - loss: 6958619.2912 - mae: 2022.7863 - val_loss: 14478401.0000 - val_mae: 3177.3459\n",
      "Epoch 93/350\n",
      "91/91 - 0s - loss: 6757163.3791 - mae: 1985.8280 - val_loss: 13977021.0000 - val_mae: 3074.1309\n",
      "Epoch 94/350\n",
      "91/91 - 0s - loss: 6724559.1703 - mae: 1976.9917 - val_loss: 13482003.0000 - val_mae: 2981.0945\n",
      "Epoch 95/350\n",
      "91/91 - 0s - loss: 6948525.4286 - mae: 2014.5865 - val_loss: 13325435.0000 - val_mae: 2973.0615\n",
      "Epoch 96/350\n",
      "91/91 - 0s - loss: 6927162.6181 - mae: 2010.5292 - val_loss: 13861469.0000 - val_mae: 3110.0203\n",
      "Epoch 97/350\n",
      "91/91 - 0s - loss: 6669423.0989 - mae: 1994.2623 - val_loss: 14531037.0000 - val_mae: 3226.3125\n",
      "Epoch 98/350\n",
      "91/91 - 0s - loss: 7642102.5110 - mae: 2113.4702 - val_loss: 12483457.0000 - val_mae: 2949.0032\n",
      "Epoch 99/350\n",
      "91/91 - 0s - loss: 7364400.1758 - mae: 2132.3159 - val_loss: 10079401.0000 - val_mae: 2574.8672\n",
      "Epoch 100/350\n",
      "91/91 - 0s - loss: 8140512.9451 - mae: 2232.6121 - val_loss: 10116712.0000 - val_mae: 2562.1858\n",
      "Epoch 101/350\n",
      "91/91 - 0s - loss: 7967539.4451 - mae: 2210.6042 - val_loss: 10291005.0000 - val_mae: 2650.8987\n",
      "Epoch 102/350\n",
      "91/91 - 0s - loss: 8335816.6868 - mae: 2271.6130 - val_loss: 10394971.0000 - val_mae: 2650.9832\n",
      "Epoch 103/350\n",
      "91/91 - 0s - loss: 7943947.1209 - mae: 2218.7380 - val_loss: 10439037.0000 - val_mae: 2592.6074\n",
      "Epoch 104/350\n",
      "91/91 - 0s - loss: 9978874.9231 - mae: 2416.2578 - val_loss: 7544181.5000 - val_mae: 2091.4485\n",
      "Epoch 105/350\n",
      "91/91 - 0s - loss: 11627954.1978 - mae: 2738.1738 - val_loss: 22988792.0000 - val_mae: 3774.4668\n",
      "Epoch 106/350\n",
      "91/91 - 0s - loss: 10628149.7473 - mae: 2665.0381 - val_loss: 15090313.0000 - val_mae: 2927.9414\n",
      "Epoch 107/350\n",
      "91/91 - 0s - loss: 10157966.7692 - mae: 2500.5403 - val_loss: 16834710.0000 - val_mae: 3014.2571\n",
      "Epoch 108/350\n",
      "91/91 - 0s - loss: 9477940.5165 - mae: 2353.4150 - val_loss: 16673915.0000 - val_mae: 3273.1140\n",
      "Epoch 109/350\n",
      "91/91 - 0s - loss: 10971767.9890 - mae: 2502.0173 - val_loss: 17851804.0000 - val_mae: 3594.3831\n",
      "Epoch 110/350\n",
      "91/91 - 0s - loss: 11867740.7363 - mae: 2645.3489 - val_loss: 12672996.0000 - val_mae: 2805.3972\n",
      "Epoch 111/350\n",
      "91/91 - 0s - loss: 11746946.4945 - mae: 2721.7339 - val_loss: 12048595.0000 - val_mae: 2659.7336\n",
      "Epoch 112/350\n",
      "91/91 - 0s - loss: 11142561.9011 - mae: 2629.3765 - val_loss: 12234484.0000 - val_mae: 2710.2566\n",
      "Epoch 113/350\n",
      "91/91 - 0s - loss: 11537389.2418 - mae: 2700.8823 - val_loss: 11018004.0000 - val_mae: 2648.3884\n",
      "Epoch 114/350\n",
      "91/91 - 0s - loss: 10775813.5495 - mae: 2564.0129 - val_loss: 10747631.0000 - val_mae: 2657.6389\n",
      "Epoch 115/350\n",
      "91/91 - 0s - loss: 10341814.9341 - mae: 2483.9902 - val_loss: 11295032.0000 - val_mae: 2703.4304\n",
      "Epoch 116/350\n",
      "91/91 - 0s - loss: 10074783.6374 - mae: 2475.2720 - val_loss: 10853308.0000 - val_mae: 2675.8625\n",
      "Epoch 117/350\n",
      "91/91 - 0s - loss: 9789932.6593 - mae: 2434.0056 - val_loss: 11081456.0000 - val_mae: 2696.0022\n",
      "Epoch 118/350\n",
      "91/91 - 0s - loss: 9429709.1319 - mae: 2373.7087 - val_loss: 10784522.0000 - val_mae: 2678.8821\n",
      "Epoch 119/350\n",
      "91/91 - 0s - loss: 9255999.6978 - mae: 2375.2683 - val_loss: 10805856.0000 - val_mae: 2678.6934\n",
      "Epoch 120/350\n",
      "91/91 - 0s - loss: 9128833.4176 - mae: 2340.3467 - val_loss: 11054483.0000 - val_mae: 2712.8777\n",
      "Epoch 121/350\n",
      "91/91 - 0s - loss: 9000946.2088 - mae: 2295.5117 - val_loss: 11321807.0000 - val_mae: 2737.6787\n",
      "Epoch 122/350\n",
      "91/91 - 0s - loss: 8830434.6593 - mae: 2284.6064 - val_loss: 11099920.0000 - val_mae: 2703.2534\n",
      "Epoch 123/350\n",
      "91/91 - 0s - loss: 8724382.0220 - mae: 2292.3259 - val_loss: 11274457.0000 - val_mae: 2727.7773\n",
      "Epoch 124/350\n",
      "91/91 - 0s - loss: 8685777.2418 - mae: 2241.5642 - val_loss: 12063240.0000 - val_mae: 2843.9861\n",
      "Epoch 125/350\n",
      "91/91 - 0s - loss: 8505783.9451 - mae: 2198.6775 - val_loss: 11403307.0000 - val_mae: 2724.1082\n",
      "Epoch 126/350\n",
      "91/91 - 0s - loss: 8340742.9780 - mae: 2212.1169 - val_loss: 11522303.0000 - val_mae: 2735.3320\n",
      "Epoch 127/350\n",
      "91/91 - 0s - loss: 8239549.6593 - mae: 2198.3701 - val_loss: 11684183.0000 - val_mae: 2757.0442\n",
      "Epoch 128/350\n",
      "91/91 - 0s - loss: 8202737.0549 - mae: 2161.3164 - val_loss: 11960484.0000 - val_mae: 2818.6316\n",
      "Epoch 129/350\n",
      "91/91 - 0s - loss: 8230057.4396 - mae: 2138.1260 - val_loss: 11800693.0000 - val_mae: 2761.1055\n",
      "Epoch 130/350\n",
      "91/91 - 0s - loss: 8522011.7363 - mae: 2249.1333 - val_loss: 11900452.0000 - val_mae: 2737.0586\n",
      "Epoch 131/350\n",
      "91/91 - 0s - loss: 8069806.8791 - mae: 2200.5750 - val_loss: 12360787.0000 - val_mae: 2912.0920\n",
      "Epoch 132/350\n",
      "91/91 - 0s - loss: 8263503.7363 - mae: 2133.7236 - val_loss: 12411651.0000 - val_mae: 2926.5876\n",
      "Epoch 133/350\n",
      "91/91 - 0s - loss: 8485840.3187 - mae: 2204.8567 - val_loss: 12072945.0000 - val_mae: 2746.7422\n",
      "Epoch 134/350\n",
      "91/91 - 0s - loss: 8229694.4890 - mae: 2157.7517 - val_loss: 12174773.0000 - val_mae: 2883.9229\n",
      "Epoch 135/350\n",
      "91/91 - 0s - loss: 7919943.1758 - mae: 2095.6316 - val_loss: 12092228.0000 - val_mae: 2858.5969\n",
      "Epoch 136/350\n",
      "91/91 - 0s - loss: 7798641.8077 - mae: 2099.4248 - val_loss: 11917473.0000 - val_mae: 2732.6914\n",
      "Epoch 137/350\n",
      "91/91 - 0s - loss: 7866717.5714 - mae: 2143.3740 - val_loss: 11917344.0000 - val_mae: 2744.7175\n",
      "Epoch 138/350\n",
      "91/91 - 0s - loss: 7760176.0989 - mae: 2108.4849 - val_loss: 12168117.0000 - val_mae: 2899.4973\n",
      "Epoch 139/350\n",
      "91/91 - 0s - loss: 8010186.2747 - mae: 2117.8926 - val_loss: 12097988.0000 - val_mae: 2885.2922\n",
      "Epoch 140/350\n",
      "91/91 - 0s - loss: 7857903.8462 - mae: 2155.0486 - val_loss: 12128764.0000 - val_mae: 2738.3035\n",
      "Epoch 141/350\n",
      "91/91 - 0s - loss: 7912802.4725 - mae: 2152.1379 - val_loss: 11885107.0000 - val_mae: 2815.7839\n",
      "Epoch 142/350\n",
      "91/91 - 0s - loss: 7682325.1374 - mae: 2075.9495 - val_loss: 12260599.0000 - val_mae: 2943.7502\n",
      "Epoch 143/350\n",
      "91/91 - 0s - loss: 7718770.7692 - mae: 2087.4011 - val_loss: 11752621.0000 - val_mae: 2749.5583\n",
      "Epoch 144/350\n",
      "91/91 - 0s - loss: 7626750.7363 - mae: 2088.5801 - val_loss: 11744233.0000 - val_mae: 2763.6963\n",
      "Epoch 145/350\n",
      "91/91 - 0s - loss: 7578213.6264 - mae: 2076.7839 - val_loss: 11792755.0000 - val_mae: 2776.9648\n",
      "Epoch 146/350\n",
      "91/91 - 0s - loss: 7573259.4066 - mae: 2070.3262 - val_loss: 11813827.0000 - val_mae: 2775.9490\n",
      "Epoch 147/350\n",
      "91/91 - 0s - loss: 7523437.6429 - mae: 2089.8550 - val_loss: 11868088.0000 - val_mae: 2712.7278\n",
      "Epoch 148/350\n",
      "91/91 - 0s - loss: 7476314.9451 - mae: 2069.1096 - val_loss: 11844860.0000 - val_mae: 2800.8655\n",
      "Epoch 149/350\n",
      "91/91 - 0s - loss: 7412290.9176 - mae: 2046.9861 - val_loss: 11842159.0000 - val_mae: 2848.8010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/350\n",
      "91/91 - 0s - loss: 7448698.2582 - mae: 2048.7627 - val_loss: 11587043.0000 - val_mae: 2758.6543\n",
      "Epoch 151/350\n",
      "91/91 - 0s - loss: 7458850.9670 - mae: 2067.1694 - val_loss: 11637540.0000 - val_mae: 2727.6428\n",
      "Epoch 152/350\n",
      "91/91 - 0s - loss: 7594961.4725 - mae: 2065.7012 - val_loss: 11878716.0000 - val_mae: 2875.4026\n",
      "Epoch 153/350\n",
      "91/91 - 0s - loss: 7549825.3681 - mae: 2072.3540 - val_loss: 11755609.0000 - val_mae: 2681.0693\n",
      "Epoch 154/350\n",
      "91/91 - 0s - loss: 7341868.6319 - mae: 2052.3557 - val_loss: 11699824.0000 - val_mae: 2805.6106\n",
      "Epoch 155/350\n",
      "91/91 - 0s - loss: 7406946.4670 - mae: 2039.1595 - val_loss: 11867215.0000 - val_mae: 2893.6707\n",
      "Epoch 156/350\n",
      "91/91 - 0s - loss: 7244746.2418 - mae: 2027.3163 - val_loss: 11530795.0000 - val_mae: 2706.7864\n",
      "Epoch 157/350\n",
      "91/91 - 0s - loss: 7294284.1703 - mae: 2046.6418 - val_loss: 11477445.0000 - val_mae: 2696.7334\n",
      "Epoch 158/350\n",
      "91/91 - 0s - loss: 7217776.8791 - mae: 2040.2898 - val_loss: 11438879.0000 - val_mae: 2717.3010\n",
      "Epoch 159/350\n",
      "91/91 - 0s - loss: 7292313.8132 - mae: 2049.7410 - val_loss: 11556848.0000 - val_mae: 2813.6589\n",
      "Epoch 160/350\n",
      "91/91 - 0s - loss: 7169880.1538 - mae: 2016.2205 - val_loss: 11519699.0000 - val_mae: 2779.1135\n",
      "Epoch 161/350\n",
      "91/91 - 0s - loss: 7157644.6813 - mae: 2024.0093 - val_loss: 11481245.0000 - val_mae: 2756.9353\n",
      "Epoch 162/350\n",
      "91/91 - 0s - loss: 7192624.9780 - mae: 2033.1339 - val_loss: 11479703.0000 - val_mae: 2784.1624\n",
      "Epoch 163/350\n",
      "91/91 - 0s - loss: 7407765.7253 - mae: 2051.9084 - val_loss: 11898783.0000 - val_mae: 2939.1423\n",
      "Epoch 164/350\n",
      "91/91 - 0s - loss: 7086809.1978 - mae: 1990.2639 - val_loss: 11341587.0000 - val_mae: 2618.7515\n",
      "Epoch 165/350\n",
      "91/91 - 0s - loss: 7337955.3901 - mae: 2077.5220 - val_loss: 11297525.0000 - val_mae: 2613.4937\n",
      "Epoch 166/350\n",
      "91/91 - 0s - loss: 7009518.3269 - mae: 2004.1466 - val_loss: 11678083.0000 - val_mae: 2906.2002\n",
      "Epoch 167/350\n",
      "91/91 - 0s - loss: 8171681.8681 - mae: 2127.3567 - val_loss: 12124456.0000 - val_mae: 3001.4080\n",
      "Epoch 168/350\n",
      "91/91 - 0s - loss: 7710285.6484 - mae: 2108.2292 - val_loss: 11618297.0000 - val_mae: 2686.9099\n",
      "Epoch 169/350\n",
      "91/91 - 0s - loss: 7501775.8901 - mae: 2093.5437 - val_loss: 10827561.0000 - val_mae: 2689.8040\n",
      "Epoch 170/350\n",
      "91/91 - 0s - loss: 6995663.3571 - mae: 1998.7672 - val_loss: 11085655.0000 - val_mae: 2813.8875\n",
      "Epoch 171/350\n",
      "91/91 - 0s - loss: 7634423.8132 - mae: 2044.9072 - val_loss: 11424069.0000 - val_mae: 2897.6946\n",
      "Epoch 172/350\n",
      "91/91 - 0s - loss: 6914772.5275 - mae: 1960.9447 - val_loss: 11289593.0000 - val_mae: 2637.6311\n",
      "Epoch 173/350\n",
      "91/91 - 0s - loss: 7447520.3242 - mae: 2101.3486 - val_loss: 11202215.0000 - val_mae: 2604.2932\n",
      "Epoch 174/350\n",
      "91/91 - 0s - loss: 7054577.6923 - mae: 2026.3575 - val_loss: 11706855.0000 - val_mae: 2920.7939\n",
      "Epoch 175/350\n",
      "91/91 - 0s - loss: 7155057.7637 - mae: 1999.2915 - val_loss: 11156163.0000 - val_mae: 2748.7188\n",
      "Epoch 176/350\n",
      "91/91 - 0s - loss: 6898824.8407 - mae: 1983.3173 - val_loss: 10941680.0000 - val_mae: 2658.9197\n",
      "Epoch 177/350\n",
      "91/91 - 0s - loss: 7183975.2692 - mae: 2016.6744 - val_loss: 10723913.0000 - val_mae: 2575.2969\n",
      "Epoch 178/350\n",
      "91/91 - 0s - loss: 6882868.7198 - mae: 1984.9709 - val_loss: 10999376.0000 - val_mae: 2806.2273\n",
      "Epoch 179/350\n",
      "91/91 - 0s - loss: 6987204.3901 - mae: 1972.1335 - val_loss: 11042919.0000 - val_mae: 2793.5774\n",
      "Epoch 180/350\n",
      "91/91 - 0s - loss: 6875915.3626 - mae: 1981.7828 - val_loss: 10816389.0000 - val_mae: 2619.5969\n",
      "Epoch 181/350\n",
      "91/91 - 0s - loss: 6867824.2308 - mae: 1991.3243 - val_loss: 10923327.0000 - val_mae: 2674.3120\n",
      "Epoch 182/350\n",
      "91/91 - 0s - loss: 6838745.8077 - mae: 1969.2424 - val_loss: 11055293.0000 - val_mae: 2770.6492\n",
      "Epoch 183/350\n",
      "91/91 - 0s - loss: 6947765.2473 - mae: 1987.8743 - val_loss: 10939742.0000 - val_mae: 2699.0334\n",
      "Epoch 184/350\n",
      "91/91 - 0s - loss: 7100552.9725 - mae: 2023.5945 - val_loss: 10956573.0000 - val_mae: 2772.8879\n",
      "Epoch 185/350\n",
      "91/91 - 0s - loss: 6816456.7637 - mae: 1975.2589 - val_loss: 10634765.0000 - val_mae: 2560.4631\n",
      "Epoch 186/350\n",
      "91/91 - 0s - loss: 7143001.6648 - mae: 2046.9532 - val_loss: 10542936.0000 - val_mae: 2645.5115\n",
      "Epoch 187/350\n",
      "91/91 - 0s - loss: 6865810.4505 - mae: 1947.7316 - val_loss: 10896763.0000 - val_mae: 2806.0879\n",
      "Epoch 188/350\n",
      "91/91 - 0s - loss: 6794429.6374 - mae: 1944.6010 - val_loss: 10557530.0000 - val_mae: 2619.7415\n",
      "Epoch 189/350\n",
      "91/91 - 0s - loss: 6863316.4176 - mae: 1996.7241 - val_loss: 10645380.0000 - val_mae: 2575.2849\n",
      "Epoch 190/350\n",
      "91/91 - 0s - loss: 6902125.8462 - mae: 1971.4194 - val_loss: 10842915.0000 - val_mae: 2776.1367\n",
      "Epoch 191/350\n",
      "91/91 - 0s - loss: 6867950.9011 - mae: 1950.5096 - val_loss: 10560165.0000 - val_mae: 2638.6248\n",
      "Epoch 192/350\n",
      "91/91 - 0s - loss: 6692814.4066 - mae: 1955.5447 - val_loss: 10570712.0000 - val_mae: 2668.6667\n",
      "Epoch 193/350\n",
      "91/91 - 0s - loss: 6722885.8242 - mae: 1960.9807 - val_loss: 10488628.0000 - val_mae: 2645.4319\n",
      "Epoch 194/350\n",
      "91/91 - 0s - loss: 6809455.7033 - mae: 1950.5178 - val_loss: 10625377.0000 - val_mae: 2716.9993\n",
      "Epoch 195/350\n",
      "91/91 - 0s - loss: 6683335.4286 - mae: 1954.8650 - val_loss: 10531620.0000 - val_mae: 2587.7834\n",
      "Epoch 196/350\n",
      "91/91 - 0s - loss: 6745281.9560 - mae: 1973.7476 - val_loss: 10496925.0000 - val_mae: 2615.9900\n",
      "Epoch 197/350\n",
      "91/91 - 0s - loss: 6632595.5110 - mae: 1921.9021 - val_loss: 10477040.0000 - val_mae: 2688.5730\n",
      "Epoch 198/350\n",
      "91/91 - 0s - loss: 6810958.1209 - mae: 1944.4716 - val_loss: 10245191.0000 - val_mae: 2625.6477\n",
      "Epoch 199/350\n",
      "91/91 - 0s - loss: 6719591.5659 - mae: 1958.9091 - val_loss: 10176595.0000 - val_mae: 2509.4778\n",
      "Epoch 200/350\n",
      "91/91 - 0s - loss: 6714261.2967 - mae: 1971.4814 - val_loss: 9994446.0000 - val_mae: 2512.0139\n",
      "Epoch 201/350\n",
      "91/91 - 0s - loss: 6875307.1099 - mae: 1930.5096 - val_loss: 10196724.0000 - val_mae: 2650.4922\n",
      "Epoch 202/350\n",
      "91/91 - 0s - loss: 6624471.5220 - mae: 1910.7129 - val_loss: 9951491.0000 - val_mae: 2500.9565\n",
      "Epoch 203/350\n",
      "91/91 - 0s - loss: 7058438.3626 - mae: 2062.8337 - val_loss: 10022163.0000 - val_mae: 2524.8887\n",
      "Epoch 204/350\n",
      "91/91 - 0s - loss: 6458852.4341 - mae: 1929.8726 - val_loss: 10633593.0000 - val_mae: 2755.8777\n",
      "Epoch 205/350\n",
      "91/91 - 0s - loss: 7014853.8352 - mae: 1970.3827 - val_loss: 10262285.0000 - val_mae: 2668.1819\n",
      "Epoch 206/350\n",
      "91/91 - 0s - loss: 6763415.3297 - mae: 1957.9701 - val_loss: 10311502.0000 - val_mae: 2588.2776\n",
      "Epoch 207/350\n",
      "91/91 - 0s - loss: 6785574.1868 - mae: 1992.8409 - val_loss: 10011993.0000 - val_mae: 2510.0029\n",
      "Epoch 208/350\n",
      "91/91 - 0s - loss: 6745410.4890 - mae: 1962.7687 - val_loss: 11182803.0000 - val_mae: 2768.6047\n",
      "Epoch 209/350\n",
      "91/91 - 0s - loss: 6936276.7363 - mae: 1990.5745 - val_loss: 9894196.0000 - val_mae: 2494.2512\n",
      "Epoch 210/350\n",
      "91/91 - 0s - loss: 7290907.7802 - mae: 2057.0210 - val_loss: 9818371.0000 - val_mae: 2518.4734\n",
      "Epoch 211/350\n",
      "91/91 - 0s - loss: 7454066.7692 - mae: 2054.8782 - val_loss: 9757556.0000 - val_mae: 2497.1699\n",
      "Epoch 212/350\n",
      "91/91 - 0s - loss: 7003870.2527 - mae: 1977.8652 - val_loss: 9567466.0000 - val_mae: 2451.1489\n",
      "Epoch 213/350\n",
      "91/91 - 0s - loss: 7644467.2857 - mae: 2121.7483 - val_loss: 12230497.0000 - val_mae: 2748.5449\n",
      "Epoch 214/350\n",
      "91/91 - 0s - loss: 7934304.6648 - mae: 2141.4666 - val_loss: 14184755.0000 - val_mae: 3066.6721\n",
      "Epoch 215/350\n",
      "91/91 - 0s - loss: 8511378.5769 - mae: 2241.9871 - val_loss: 12498540.0000 - val_mae: 2863.6602\n",
      "Epoch 216/350\n",
      "91/91 - 0s - loss: 9744406.7473 - mae: 2433.2844 - val_loss: 16178435.0000 - val_mae: 3507.3259\n",
      "Epoch 217/350\n",
      "91/91 - 0s - loss: 10077336.5934 - mae: 2369.4431 - val_loss: 25254128.0000 - val_mae: 4344.3345\n",
      "Epoch 218/350\n",
      "91/91 - 0s - loss: 10484670.9670 - mae: 2487.8479 - val_loss: 13664019.0000 - val_mae: 3202.7019\n",
      "Epoch 219/350\n",
      "91/91 - 0s - loss: 8632961.0824 - mae: 2358.8210 - val_loss: 11290632.0000 - val_mae: 2677.5518\n",
      "Epoch 220/350\n",
      "91/91 - 0s - loss: 9467151.3022 - mae: 2486.2703 - val_loss: 14072973.0000 - val_mae: 3147.3801\n",
      "Epoch 221/350\n",
      "91/91 - 0s - loss: 9118974.9670 - mae: 2367.9602 - val_loss: 16255005.0000 - val_mae: 3412.9910\n",
      "Epoch 222/350\n",
      "91/91 - 0s - loss: 9678168.2418 - mae: 2451.8511 - val_loss: 10156746.0000 - val_mae: 2591.7019\n",
      "Epoch 223/350\n",
      "91/91 - 0s - loss: 10497143.0879 - mae: 2590.1577 - val_loss: 12138676.0000 - val_mae: 2866.7249\n",
      "Epoch 224/350\n",
      "91/91 - 0s - loss: 11529114.0220 - mae: 2646.9707 - val_loss: 16355796.0000 - val_mae: 2962.0371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/350\n",
      "91/91 - 0s - loss: 12844075.6484 - mae: 2903.2520 - val_loss: 16121287.0000 - val_mae: 3054.5273\n",
      "Epoch 226/350\n",
      "91/91 - 0s - loss: 12160884.3516 - mae: 2733.1377 - val_loss: 15013176.0000 - val_mae: 3023.5957\n",
      "Epoch 227/350\n",
      "91/91 - 0s - loss: 11053434.9890 - mae: 2680.9805 - val_loss: 13870525.0000 - val_mae: 2938.6575\n",
      "Epoch 228/350\n",
      "91/91 - 0s - loss: 9942310.3516 - mae: 2540.6072 - val_loss: 13431095.0000 - val_mae: 2965.8477\n",
      "Epoch 229/350\n",
      "91/91 - 0s - loss: 9660214.9121 - mae: 2413.9343 - val_loss: 13957229.0000 - val_mae: 3075.6223\n",
      "Epoch 230/350\n",
      "91/91 - 0s - loss: 9219889.7912 - mae: 2386.1709 - val_loss: 13857219.0000 - val_mae: 2968.9929\n",
      "Epoch 231/350\n",
      "91/91 - 0s - loss: 8292529.0440 - mae: 2297.5864 - val_loss: 13420149.0000 - val_mae: 2893.3904\n",
      "Epoch 232/350\n",
      "91/91 - 0s - loss: 8464638.4560 - mae: 2262.6104 - val_loss: 13470789.0000 - val_mae: 2984.3828\n",
      "Epoch 233/350\n",
      "91/91 - 0s - loss: 8564636.9560 - mae: 2234.0422 - val_loss: 12992143.0000 - val_mae: 2884.7898\n",
      "Epoch 234/350\n",
      "91/91 - 0s - loss: 8252145.4945 - mae: 2227.6931 - val_loss: 13347117.0000 - val_mae: 2852.8586\n",
      "Epoch 235/350\n",
      "91/91 - 0s - loss: 9119723.2857 - mae: 2417.8613 - val_loss: 14378887.0000 - val_mae: 3035.1270\n",
      "Epoch 236/350\n",
      "91/91 - 0s - loss: 8654274.3077 - mae: 2395.2334 - val_loss: 14234787.0000 - val_mae: 2974.5906\n",
      "Epoch 237/350\n",
      "91/91 - 0s - loss: 8887374.7582 - mae: 2261.1204 - val_loss: 14364036.0000 - val_mae: 2919.1094\n",
      "Epoch 238/350\n",
      "91/91 - 0s - loss: 8886712.9011 - mae: 2350.6340 - val_loss: 14138728.0000 - val_mae: 3018.5693\n",
      "Epoch 239/350\n",
      "91/91 - 0s - loss: 8964178.5824 - mae: 2419.6194 - val_loss: 13803785.0000 - val_mae: 2973.1868\n",
      "Epoch 240/350\n",
      "91/91 - 0s - loss: 8906955.2912 - mae: 2395.9438 - val_loss: 13801316.0000 - val_mae: 2928.2141\n",
      "Epoch 241/350\n",
      "91/91 - 0s - loss: 9203465.5934 - mae: 2371.6338 - val_loss: 13850649.0000 - val_mae: 2909.0049\n",
      "Epoch 242/350\n",
      "91/91 - 0s - loss: 9106560.4176 - mae: 2368.0979 - val_loss: 13883949.0000 - val_mae: 3020.6921\n",
      "Epoch 243/350\n",
      "91/91 - 0s - loss: 8497084.0330 - mae: 2329.8528 - val_loss: 13659131.0000 - val_mae: 2926.0203\n",
      "Epoch 244/350\n",
      "91/91 - 0s - loss: 8379051.2308 - mae: 2226.0349 - val_loss: 14495681.0000 - val_mae: 2958.8591\n",
      "Epoch 245/350\n",
      "91/91 - 0s - loss: 8430938.7802 - mae: 2217.5701 - val_loss: 13558623.0000 - val_mae: 2905.6340\n",
      "Epoch 246/350\n",
      "91/91 - 0s - loss: 7863116.9231 - mae: 2195.5073 - val_loss: 13337784.0000 - val_mae: 2873.8831\n",
      "Epoch 247/350\n",
      "91/91 - 0s - loss: 7702441.0385 - mae: 2147.9985 - val_loss: 13350069.0000 - val_mae: 2864.6951\n",
      "Epoch 248/350\n",
      "91/91 - 0s - loss: 7600125.1978 - mae: 2099.6648 - val_loss: 12997681.0000 - val_mae: 2831.5159\n",
      "Epoch 249/350\n",
      "91/91 - 0s - loss: 7472751.4725 - mae: 2084.4224 - val_loss: 12697856.0000 - val_mae: 2791.4961\n",
      "Epoch 250/350\n",
      "91/91 - 0s - loss: 7374335.7198 - mae: 2092.3955 - val_loss: 12564239.0000 - val_mae: 2784.4827\n",
      "Epoch 251/350\n",
      "91/91 - 0s - loss: 7279324.7088 - mae: 2071.9487 - val_loss: 12438867.0000 - val_mae: 2779.7996\n",
      "Epoch 252/350\n",
      "91/91 - 0s - loss: 7291359.3846 - mae: 2069.2236 - val_loss: 12306333.0000 - val_mae: 2770.9744\n",
      "Epoch 253/350\n",
      "91/91 - 0s - loss: 7123847.1538 - mae: 2040.6880 - val_loss: 12213972.0000 - val_mae: 2767.5898\n",
      "Epoch 254/350\n",
      "91/91 - 0s - loss: 7272597.1209 - mae: 2060.2507 - val_loss: 10398941.0000 - val_mae: 2459.9392\n",
      "Epoch 255/350\n",
      "91/91 - 0s - loss: 7197035.7418 - mae: 2044.8645 - val_loss: 9800976.0000 - val_mae: 2391.1592\n",
      "Epoch 256/350\n",
      "91/91 - 0s - loss: 7468759.5714 - mae: 2105.4204 - val_loss: 10178189.0000 - val_mae: 2438.5605\n",
      "Epoch 257/350\n",
      "91/91 - 0s - loss: 7529696.7253 - mae: 2117.7031 - val_loss: 11825537.0000 - val_mae: 2740.2371\n",
      "Epoch 258/350\n",
      "91/91 - 0s - loss: 7968136.4396 - mae: 2089.8330 - val_loss: 11820531.0000 - val_mae: 2883.2727\n",
      "Epoch 259/350\n",
      "91/91 - 0s - loss: 9007204.7967 - mae: 2390.6658 - val_loss: 16205387.0000 - val_mae: 3369.1904\n",
      "Epoch 260/350\n",
      "91/91 - 0s - loss: 9218201.0000 - mae: 2473.5859 - val_loss: 15453821.0000 - val_mae: 3256.4932\n",
      "Epoch 261/350\n",
      "91/91 - 0s - loss: 8370889.3791 - mae: 2296.2983 - val_loss: 14019853.0000 - val_mae: 3033.3225\n",
      "Epoch 262/350\n",
      "91/91 - 0s - loss: 8337561.2253 - mae: 2252.7290 - val_loss: 12738635.0000 - val_mae: 2844.5930\n",
      "Epoch 263/350\n",
      "91/91 - 0s - loss: 7841864.5714 - mae: 2211.2109 - val_loss: 13308915.0000 - val_mae: 2986.9851\n",
      "Epoch 264/350\n",
      "91/91 - 0s - loss: 7653666.0495 - mae: 2166.7476 - val_loss: 13584397.0000 - val_mae: 2997.5051\n",
      "Epoch 265/350\n",
      "91/91 - 0s - loss: 7919581.9011 - mae: 2226.0286 - val_loss: 12769976.0000 - val_mae: 2944.0879\n",
      "Epoch 266/350\n",
      "91/91 - 0s - loss: 8049731.4505 - mae: 2244.2949 - val_loss: 13344196.0000 - val_mae: 2867.3406\n",
      "Epoch 267/350\n",
      "91/91 - 0s - loss: 8014378.1758 - mae: 2253.4944 - val_loss: 12062272.0000 - val_mae: 2889.7537\n",
      "Epoch 268/350\n",
      "91/91 - 0s - loss: 7591447.0330 - mae: 2188.1628 - val_loss: 12668269.0000 - val_mae: 2788.8975\n",
      "Epoch 269/350\n",
      "91/91 - 0s - loss: 7462618.3736 - mae: 2112.4487 - val_loss: 12197461.0000 - val_mae: 2805.4607\n",
      "Epoch 270/350\n",
      "91/91 - 0s - loss: 7357417.2527 - mae: 2117.8806 - val_loss: 11718176.0000 - val_mae: 2783.6917\n",
      "Epoch 271/350\n",
      "91/91 - 0s - loss: 7393230.2747 - mae: 2132.6675 - val_loss: 9744017.0000 - val_mae: 2531.1384\n",
      "Epoch 272/350\n",
      "91/91 - 0s - loss: 7338572.7692 - mae: 2111.2886 - val_loss: 10322619.0000 - val_mae: 2625.1370\n",
      "Epoch 273/350\n",
      "91/91 - 0s - loss: 7344506.7473 - mae: 2097.8867 - val_loss: 9725799.0000 - val_mae: 2537.1416\n",
      "Epoch 274/350\n",
      "91/91 - 0s - loss: 7233962.4066 - mae: 2086.9724 - val_loss: 9768679.0000 - val_mae: 2523.8562\n",
      "Epoch 275/350\n",
      "91/91 - 0s - loss: 7075139.3681 - mae: 2088.5024 - val_loss: 10331956.0000 - val_mae: 2607.6516\n",
      "Epoch 276/350\n",
      "91/91 - 0s - loss: 7239699.5934 - mae: 2095.2590 - val_loss: 10430696.0000 - val_mae: 2584.6731\n",
      "Epoch 277/350\n",
      "91/91 - 0s - loss: 7224205.4725 - mae: 2119.6201 - val_loss: 11473139.0000 - val_mae: 2708.2131\n",
      "Epoch 278/350\n",
      "91/91 - 0s - loss: 7219006.5714 - mae: 2095.5681 - val_loss: 11364415.0000 - val_mae: 2748.1721\n",
      "Epoch 279/350\n",
      "91/91 - 0s - loss: 7111740.4231 - mae: 2093.6003 - val_loss: 10520039.0000 - val_mae: 2693.9763\n",
      "Epoch 280/350\n",
      "91/91 - 0s - loss: 7147163.0934 - mae: 2082.4731 - val_loss: 10223923.0000 - val_mae: 2620.0146\n",
      "Epoch 281/350\n",
      "91/91 - 0s - loss: 6985449.1868 - mae: 2034.7180 - val_loss: 10440969.0000 - val_mae: 2683.3223\n",
      "Epoch 282/350\n",
      "91/91 - 0s - loss: 7045545.4396 - mae: 2050.9661 - val_loss: 10692497.0000 - val_mae: 2680.2971\n",
      "Epoch 283/350\n",
      "91/91 - 0s - loss: 7348099.2857 - mae: 2104.0659 - val_loss: 10167545.0000 - val_mae: 2589.2952\n",
      "Epoch 284/350\n",
      "91/91 - 0s - loss: 7335625.5934 - mae: 2107.1079 - val_loss: 12300667.0000 - val_mae: 2897.4197\n",
      "Epoch 285/350\n",
      "91/91 - 0s - loss: 7203724.2857 - mae: 2030.0227 - val_loss: 12967695.0000 - val_mae: 3040.1523\n",
      "Epoch 286/350\n",
      "91/91 - 0s - loss: 7349303.5495 - mae: 2089.2688 - val_loss: 12534180.0000 - val_mae: 2949.4797\n",
      "Epoch 287/350\n",
      "91/91 - 0s - loss: 7750090.3571 - mae: 2220.5095 - val_loss: 12538671.0000 - val_mae: 2971.4055\n",
      "Epoch 288/350\n",
      "91/91 - 0s - loss: 7899488.1099 - mae: 2174.9475 - val_loss: 14538565.0000 - val_mae: 3263.3223\n",
      "Epoch 289/350\n",
      "91/91 - 0s - loss: 8916226.2308 - mae: 2283.5479 - val_loss: 12822883.0000 - val_mae: 3050.0667\n",
      "Epoch 290/350\n",
      "91/91 - 0s - loss: 9311295.5330 - mae: 2403.3877 - val_loss: 12526649.0000 - val_mae: 2950.4905\n",
      "Epoch 291/350\n",
      "91/91 - 0s - loss: 8077666.2088 - mae: 2228.7764 - val_loss: 14143456.0000 - val_mae: 3228.3982\n",
      "Epoch 292/350\n",
      "91/91 - 0s - loss: 8195581.3791 - mae: 2233.5618 - val_loss: 12581645.0000 - val_mae: 3027.8508\n",
      "Epoch 293/350\n",
      "91/91 - 0s - loss: 8030996.3791 - mae: 2228.4771 - val_loss: 12264617.0000 - val_mae: 2910.5071\n",
      "Epoch 294/350\n",
      "91/91 - 0s - loss: 7786516.5549 - mae: 2211.6670 - val_loss: 13017785.0000 - val_mae: 3083.5215\n",
      "Epoch 295/350\n",
      "91/91 - 0s - loss: 7744650.0989 - mae: 2152.5972 - val_loss: 12910763.0000 - val_mae: 3066.4255\n",
      "Epoch 296/350\n",
      "91/91 - 0s - loss: 7607088.6429 - mae: 2157.4509 - val_loss: 12178149.0000 - val_mae: 2940.1211\n",
      "Epoch 297/350\n",
      "91/91 - 0s - loss: 7767502.3516 - mae: 2196.0520 - val_loss: 12195843.0000 - val_mae: 2940.1648\n",
      "Epoch 298/350\n",
      "91/91 - 0s - loss: 7581838.2363 - mae: 2157.1978 - val_loss: 13594531.0000 - val_mae: 3170.7864\n",
      "Epoch 299/350\n",
      "91/91 - 0s - loss: 7723043.5934 - mae: 2132.2542 - val_loss: 12447609.0000 - val_mae: 2982.0049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/350\n",
      "91/91 - 0s - loss: 7506815.3626 - mae: 2156.9404 - val_loss: 12335209.0000 - val_mae: 2893.5869\n",
      "Epoch 301/350\n",
      "91/91 - 0s - loss: 7803335.5275 - mae: 2207.3083 - val_loss: 12369432.0000 - val_mae: 2985.0723\n",
      "Epoch 302/350\n",
      "91/91 - 0s - loss: 7366902.6484 - mae: 2078.9104 - val_loss: 12706623.0000 - val_mae: 3079.6758\n",
      "Epoch 303/350\n",
      "91/91 - 0s - loss: 7379393.2143 - mae: 2093.4456 - val_loss: 12005364.0000 - val_mae: 2889.0410\n",
      "Epoch 304/350\n",
      "91/91 - 0s - loss: 7366142.1868 - mae: 2117.5161 - val_loss: 12271364.0000 - val_mae: 2981.8005\n",
      "Epoch 305/350\n",
      "91/91 - 0s - loss: 7246437.1484 - mae: 2071.1904 - val_loss: 12085736.0000 - val_mae: 2932.3425\n",
      "Epoch 306/350\n",
      "91/91 - 0s - loss: 7241350.8132 - mae: 2057.2930 - val_loss: 12008728.0000 - val_mae: 2921.4187\n",
      "Epoch 307/350\n",
      "91/91 - 0s - loss: 7148119.0385 - mae: 2073.9485 - val_loss: 11495231.0000 - val_mae: 2776.0703\n",
      "Epoch 308/350\n",
      "91/91 - 0s - loss: 7252376.3297 - mae: 2118.1150 - val_loss: 11590149.0000 - val_mae: 2837.7073\n",
      "Epoch 309/350\n",
      "91/91 - 0s - loss: 7292974.0055 - mae: 2097.4402 - val_loss: 11678741.0000 - val_mae: 2882.0569\n",
      "Epoch 310/350\n",
      "91/91 - 0s - loss: 7603091.5385 - mae: 2132.1658 - val_loss: 11953877.0000 - val_mae: 2962.1091\n",
      "Epoch 311/350\n",
      "91/91 - 0s - loss: 7146338.6813 - mae: 2091.7654 - val_loss: 12290713.0000 - val_mae: 2867.0281\n",
      "Epoch 312/350\n",
      "91/91 - 0s - loss: 7684433.6154 - mae: 2226.9551 - val_loss: 12885973.0000 - val_mae: 3048.1365\n",
      "Epoch 313/350\n",
      "91/91 - 0s - loss: 7618696.3077 - mae: 2177.4348 - val_loss: 13803692.0000 - val_mae: 3213.4160\n",
      "Epoch 314/350\n",
      "91/91 - 0s - loss: 7727504.9890 - mae: 2153.0542 - val_loss: 12860837.0000 - val_mae: 3035.5723\n",
      "Epoch 315/350\n",
      "91/91 - 0s - loss: 7751342.6374 - mae: 2214.7266 - val_loss: 12831413.0000 - val_mae: 3020.3455\n",
      "Epoch 316/350\n",
      "91/91 - 0s - loss: 7634547.1319 - mae: 2189.2371 - val_loss: 13419029.0000 - val_mae: 3161.0642\n",
      "Epoch 317/350\n",
      "91/91 - 0s - loss: 7725939.0659 - mae: 2168.2195 - val_loss: 13652933.0000 - val_mae: 3202.8074\n",
      "Epoch 318/350\n",
      "91/91 - 0s - loss: 7709214.3077 - mae: 2179.5205 - val_loss: 12940659.0000 - val_mae: 3036.8435\n",
      "Epoch 319/350\n",
      "91/91 - 0s - loss: 7679897.8791 - mae: 2197.9368 - val_loss: 13202747.0000 - val_mae: 3124.0657\n",
      "Epoch 320/350\n",
      "91/91 - 0s - loss: 7581570.4505 - mae: 2155.6443 - val_loss: 13129359.0000 - val_mae: 3109.6746\n",
      "Epoch 321/350\n",
      "91/91 - 0s - loss: 7610312.7802 - mae: 2193.1653 - val_loss: 12916643.0000 - val_mae: 3044.0732\n",
      "Epoch 322/350\n",
      "91/91 - 0s - loss: 7698806.9945 - mae: 2208.6604 - val_loss: 13238745.0000 - val_mae: 3148.9307\n",
      "Epoch 323/350\n",
      "91/91 - 0s - loss: 7627047.4890 - mae: 2175.0225 - val_loss: 12468101.0000 - val_mae: 2966.4172\n",
      "Epoch 324/350\n",
      "91/91 - 0s - loss: 7663145.8022 - mae: 2207.5867 - val_loss: 12315951.0000 - val_mae: 2944.5496\n",
      "Epoch 325/350\n",
      "91/91 - 0s - loss: 7472905.4615 - mae: 2158.2942 - val_loss: 12325856.0000 - val_mae: 2969.4209\n",
      "Epoch 326/350\n",
      "91/91 - 0s - loss: 7574946.0604 - mae: 2148.0784 - val_loss: 12703124.0000 - val_mae: 3070.5676\n",
      "Epoch 327/350\n",
      "91/91 - 0s - loss: 7455654.4505 - mae: 2138.2009 - val_loss: 11996889.0000 - val_mae: 2864.8984\n",
      "Epoch 328/350\n",
      "91/91 - 0s - loss: 7545476.9451 - mae: 2187.6189 - val_loss: 12028569.0000 - val_mae: 2874.2998\n",
      "Epoch 329/350\n",
      "91/91 - 0s - loss: 7688639.3626 - mae: 2181.7590 - val_loss: 13239017.0000 - val_mae: 3151.9792\n",
      "Epoch 330/350\n",
      "91/91 - 0s - loss: 7552547.8626 - mae: 2114.6497 - val_loss: 12112061.0000 - val_mae: 2887.8555\n",
      "Epoch 331/350\n",
      "91/91 - 0s - loss: 7625508.4121 - mae: 2210.0610 - val_loss: 12135405.0000 - val_mae: 2817.8132\n",
      "Epoch 332/350\n",
      "91/91 - 0s - loss: 7419209.8626 - mae: 2191.7556 - val_loss: 12704021.0000 - val_mae: 3067.3752\n",
      "Epoch 333/350\n",
      "91/91 - 0s - loss: 7542731.3187 - mae: 2139.8718 - val_loss: 13328707.0000 - val_mae: 3162.4226\n",
      "Epoch 334/350\n",
      "91/91 - 0s - loss: 7393362.3297 - mae: 2106.8660 - val_loss: 11775772.0000 - val_mae: 2825.3625\n",
      "Epoch 335/350\n",
      "91/91 - 0s - loss: 7424776.2198 - mae: 2180.0378 - val_loss: 11752623.0000 - val_mae: 2813.2395\n",
      "Epoch 336/350\n",
      "91/91 - 0s - loss: 7318212.1703 - mae: 2144.1768 - val_loss: 12375925.0000 - val_mae: 3023.9531\n",
      "Epoch 337/350\n",
      "91/91 - 0s - loss: 7649846.1484 - mae: 2151.0720 - val_loss: 12427424.0000 - val_mae: 3037.4861\n",
      "Epoch 338/350\n",
      "91/91 - 0s - loss: 7294621.3736 - mae: 2109.7490 - val_loss: 11655435.0000 - val_mae: 2823.8955\n",
      "Epoch 339/350\n",
      "91/91 - 0s - loss: 7297389.0165 - mae: 2147.5962 - val_loss: 11253101.0000 - val_mae: 2828.0830\n",
      "Epoch 340/350\n",
      "91/91 - 0s - loss: 7167201.9945 - mae: 2099.6482 - val_loss: 11756088.0000 - val_mae: 2969.8828\n",
      "Epoch 341/350\n",
      "91/91 - 0s - loss: 7251344.1154 - mae: 2094.8728 - val_loss: 11558033.0000 - val_mae: 2923.6130\n",
      "Epoch 342/350\n",
      "91/91 - 0s - loss: 7218553.6868 - mae: 2114.4170 - val_loss: 11151505.0000 - val_mae: 2793.0632\n",
      "Epoch 343/350\n",
      "91/91 - 0s - loss: 7232080.1209 - mae: 2137.2891 - val_loss: 11210797.0000 - val_mae: 2813.2878\n",
      "Epoch 344/350\n",
      "91/91 - 0s - loss: 7474970.4780 - mae: 2152.4390 - val_loss: 11932860.0000 - val_mae: 2979.5144\n",
      "Epoch 345/350\n",
      "91/91 - 0s - loss: 7276708.7143 - mae: 2122.7690 - val_loss: 10981072.0000 - val_mae: 2733.2405\n",
      "Epoch 346/350\n",
      "91/91 - 0s - loss: 7203709.5165 - mae: 2132.6345 - val_loss: 11657219.0000 - val_mae: 2781.6106\n",
      "Epoch 347/350\n",
      "91/91 - 0s - loss: 7038468.3791 - mae: 2101.4119 - val_loss: 11825803.0000 - val_mae: 2823.4172\n",
      "Epoch 348/350\n",
      "91/91 - 0s - loss: 7001933.0495 - mae: 2115.8469 - val_loss: 12007815.0000 - val_mae: 2872.3730\n",
      "Epoch 349/350\n",
      "91/91 - 0s - loss: 7010383.6868 - mae: 2115.9609 - val_loss: 11929104.0000 - val_mae: 2809.7786\n",
      "Epoch 350/350\n",
      "91/91 - 0s - loss: 7183439.2088 - mae: 2116.6746 - val_loss: 11077377.0000 - val_mae: 2771.3848\n",
      "MSE: 11077377.000, RMSE: 3328.269, MAE: 2771.385\n",
      "Predicted: 16858.299\n"
     ]
    }
   ],
   "source": [
    "# lstm for time series forecasting\n",
    "from numpy import sqrt\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return asarray(X), asarray(y)\n",
    "\n",
    "# load the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-car-sales.csv'\n",
    "df = read_csv(path, header=0, index_col=0, squeeze=True)\n",
    "\n",
    "# retrieve the values\n",
    "values = df.values.astype('float32')\n",
    "\n",
    "# specify the window size\n",
    "n_steps = 5\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(values, n_steps)\n",
    "\n",
    "# reshape into [samples, timesteps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# split into train/test\n",
    "n_test = 12\n",
    "X_train, X_test, y_train, y_test = X[:-n_test], X[-n_test:], y[:-n_test], y[-n_test:]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', kernel_initializer='he_normal', input_shape=(n_steps,1)))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=350, batch_size=32, verbose=2, validation_data=(X_test, y_test))\n",
    "\n",
    "# evaluate the model\n",
    "mse, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('MSE: %.3f, RMSE: %.3f, MAE: %.3f' % (mse, sqrt(mse), mae))\n",
    "\n",
    "# make a prediction\n",
    "row = asarray([18024.0, 16722.0, 14385.0, 21342.0, 17180.0]).reshape((1, n_steps, 1))\n",
    "yhat = model.predict(row)\n",
    "print('Predicted: %.3f' % (yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
